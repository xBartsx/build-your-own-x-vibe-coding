#!/usr/bin/env python3
"""
Check Loop 1: Tokenizer implementation
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_tokenizer():
    print("ğŸ” Checking Tokenizer implementation...\n")
    
    try:
        # å°è¯•å¯¼å…¥ç”¨æˆ·çš„ tokenizer
        from tokenizer import CharTokenizer
        print("âœ… Found CharTokenizer class")
    except ImportError as e:
        print("âŒ Cannot import CharTokenizer")
        print("Hint: Make sure you have a tokenizer.py file with CharTokenizer class")
        return False
    
    # æµ‹è¯• 1: åˆ›å»º tokenizer
    try:
        tokenizer = CharTokenizer()
        print("âœ… Tokenizer created successfully")
        print(f"   Vocab size: {tokenizer.vocab_size}")
    except Exception as e:
        print(f"âŒ Failed to create Tokenizer: {e}")
        return False
    
    # æµ‹è¯• 2: encode åŠŸèƒ½
    test_text = "hello"
    try:
        tokens = tokenizer.encode(test_text)
        print(f"âœ… Encode successful: '{test_text}' -> {tokens}")
        
        # Check if it's a list of numbers
        if not isinstance(tokens, (list, tuple)) or not all(isinstance(t, int) for t in tokens):
            print("âš ï¸  Warning: encode should return a list of integers")
    except Exception as e:
        print(f"âŒ Encode failed: {e}")
        return False
    
    # æµ‹è¯• 3: decode åŠŸèƒ½
    try:
        decoded = tokenizer.decode(tokens)
        print(f"âœ… Decode successful: {tokens} -> '{decoded}'")
        
        if decoded == test_text:
            print("âœ… Perfect! Encode/Decode round trip consistent")
        else:
            print(f"âš ï¸  Warning: decoded result '{decoded}' doesn't match original '{test_text}'")
    except Exception as e:
        print(f"âŒ Decode failed: {e}")
        return False
    
    # æµ‹è¯• 4: ç‰¹æ®Š token
    print("\nğŸ” Checking special tokens...")
    special_tokens = ['<pad>', '<unk>', '<eos>']
    found = 0
    
    for token in special_tokens:
        if hasattr(tokenizer, 'char_to_id') and token in tokenizer.char_to_id:
            print(f"âœ… Found special token: {token}")
            found += 1
        elif hasattr(tokenizer, 'vocab') and token in tokenizer.vocab:
            print(f"âœ… Found special token: {token}")
            found += 1
    
    if found == 0:
        print("âš ï¸  No special tokens found, but basic functionality is not affected")
    
    # æµ‹è¯• 5: æœªçŸ¥å­—ç¬¦å¤„ç†
    print("\nğŸ” Testing unknown character handling...")
    try:
        # ä½¿ç”¨ä¸€ä¸ªå¯èƒ½ä¸åœ¨ vocab ä¸­çš„å­—ç¬¦
        weird_text = "helloğŸ˜€world"  # åŒ…å« emoji
        tokens = tokenizer.encode(weird_text)
        decoded = tokenizer.decode(tokens)
        print(f"âœ… Handling unknown characters didn't crash")
    except Exception as e:
        print(f"âš ï¸  Error handling unknown characters, but this is normal: {e}")
    
    print("\n" + "="*50)
    print("ğŸ‰ Loop 1 Complete! Tokenizer basic functionality working!")
    print("\nNext: Enter Loop 2 - Transformer")
    return True

if __name__ == "__main__":
    success = test_tokenizer()
    sys.exit(0 if success else 1)